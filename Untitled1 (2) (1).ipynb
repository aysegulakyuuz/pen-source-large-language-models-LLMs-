{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch scikit-learn\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (4.48.0)Note: you may need to restart the kernel to use updated packages.\n\nRequirement already satisfied: datasets in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (3.2.0)\nRequirement already satisfied: torch in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (2.5.1)\nRequirement already satisfied: scikit-learn in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (1.5.1)\nRequirement already satisfied: filelock in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\nRequirement already satisfied: numpy>=1.17 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\nRequirement already satisfied: networkx in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\nRequirement already satisfied: setuptools in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\nRequirement already satisfied: sympy==1.13.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.6.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: colorama in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in c:\\users\\aysegul\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {},
      "id": "9da94e0f-18dd-4a9d-a993-dfb5fdff141c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Libraries loaded successfully!\n"
        }
      ],
      "execution_count": 2,
      "metadata": {},
      "id": "b9a12572-8a4c-41ba-9fc8-2349d420e535"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GoEmotions dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"go_emotions\")\n",
        "\n",
        "# Use a small subset of the dataset\n",
        "train_data = dataset[\"train\"].select(range(5000))  # 1000 samples for training\n",
        "test_data = dataset[\"test\"].select(range(1000))    # 200 samples for testing\n",
        "\n",
        "print(f\"Training dataset size: {len(train_data)}\")\n",
        "print(f\"Test dataset size: {len(test_data)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset...\nTraining dataset size: 5000\nTest dataset size: 1000\n"
        }
      ],
      "execution_count": 3,
      "metadata": {},
      "id": "4450e232-b08f-499d-a05e-0644bc23e223"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset by extracting text and labels\n",
        "def preprocess_data(batch):\n",
        "    return {\"text\": batch[\"text\"], \"labels\": batch[\"labels\"]}\n",
        "\n",
        "# Apply preprocessing to training and testing datasets\n",
        "train_data = train_data.map(preprocess_data)\n",
        "test_data = test_data.map(preprocess_data)\n",
        "\n",
        "print(\"Dataset preprocessed successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset preprocessed successfully!\n"
        }
      ],
      "execution_count": 4,
      "metadata": {},
      "id": "1d7d52c9-a610-4909-b4d0-98e3fe724211"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DistilBERT tokenizer and model\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=28  # Number of emotion labels in GoEmotions dataset\n",
        ")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading model and tokenizer...\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model and tokenizer loaded successfully!\n"
        }
      ],
      "execution_count": 5,
      "metadata": {},
      "id": "e9f6c9bf-c360-4cec-bbbf-71b278b80f24"
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset by extracting text and the first label (single-label classification)\n",
        "def preprocess_data(batch):\n",
        "    # Select only the first label for each sample (multi-label to single-label)\n",
        "    return {\"text\": batch[\"text\"], \"labels\": batch[\"labels\"][0] if len(batch[\"labels\"]) > 0 else 0}\n",
        "\n",
        "# Apply preprocessing to training and testing datasets\n",
        "train_data = train_data.map(preprocess_data)\n",
        "test_data = test_data.map(preprocess_data)\n",
        "\n",
        "print(\"Dataset preprocessed for single-label classification!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset preprocessed for single-label classification!\n"
        }
      ],
      "execution_count": 6,
      "metadata": {},
      "id": "44c6ee50-0680-4f3e-86e5-5d13518fdfae"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_data(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "# Apply tokenization to training and testing datasets\n",
        "train_data = train_data.map(tokenize_data, batched=True)\n",
        "test_data = test_data.map(tokenize_data, batched=True)\n",
        "\n",
        "print(\"Datasets tokenized successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datasets tokenized successfully!\n"
        }
      ],
      "execution_count": 7,
      "metadata": {},
      "id": "9f763bab-df29-45da-afdd-22b54390358a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datasets to PyTorch format\n",
        "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print(\"Datasets converted to PyTorch format!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datasets converted to PyTorch format!\n"
        }
      ],
      "execution_count": 8,
      "metadata": {},
      "id": "b623a556-5683-4943-acdf-13eef5f22eec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Output directory for model checkpoints\n",
        "    evaluation_strategy=\"epoch\",       # Evaluate after each epoch\n",
        "    num_train_epochs=3,                # Train for 1 epoch (adjust if needed)\n",
        "    per_device_train_batch_size=8,     # Batch size per device\n",
        "    save_total_limit=1,                # Keep only the most recent checkpoint\n",
        "    logging_dir=\"./logs\",              # Directory for logs\n",
        "    logging_steps=10,    # Log every 10 steps\n",
        "    learning_rate=3e-5 \n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                       # The model to train\n",
        "    args=training_args,                # Training arguments\n",
        "    train_dataset=train_data,          # Training dataset\n",
        "    eval_dataset=test_data,            # Evaluation dataset\n",
        "    tokenizer=tokenizer                # Tokenizer for preprocessing\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "C:\\Users\\aysegul\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nC:\\Users\\aysegul\\AppData\\Local\\Temp\\ipykernel_3396\\4055551088.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting model training...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 1:17:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.825000</td>\n      <td>1.847217</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.532700</td>\n      <td>1.668983</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.008600</td>\n      <td>1.686018</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "TrainOutput(global_step=1875, training_loss=1.6739320167541505, metrics={'train_runtime': 4666.3266, 'train_samples_per_second': 3.215, 'train_steps_per_second': 0.402, 'total_flos': 194494326729984.0, 'train_loss': 1.6739320167541505, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {},
      "id": "bfdfae3b-444b-4b2f-a21e-73df96ce9d37"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define a custom metric function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    accuracy = accuracy_score(labels, predictions.numpy())\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {},
      "id": "a1fa6559-8f22-4c45-9d92-0f50a6c22a5a"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Add custom metric function\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "C:\\Users\\aysegul\\AppData\\Local\\Temp\\ipykernel_3396\\96442585.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        }
      ],
      "execution_count": 22,
      "metadata": {},
      "id": "de8b9c15-917b-4284-95d3-7e41a88841b5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "print(\"Evaluating the model...\")\n",
        "results = trainer.evaluate()\n",
        "print(f\"Validation Loss: {results['eval_loss']}\")\n",
        "print(f\"Accuracy: {results['eval_accuracy']}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Evaluating the model...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:56]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validation Loss: 1.6860178709030151\nAccuracy: 0.524\n"
        }
      ],
      "execution_count": 24,
      "metadata": {},
      "id": "96a00afa-df46-44c9-97da-235d4f5d1b71"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(\"./emotion_recognition_model\")\n",
        "tokenizer.save_pretrained(\"./emotion_recognition_model\")\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the trained model for inference\n",
        "classifier = pipeline(\"text-classification\", model=\"./emotion_recognition_model\", tokenizer=\"./emotion_recognition_model\")\n",
        "\n",
        "# Test with example sentences\n",
        "print(classifier(\"I am so happy today!\"))  # Positive emotion\n",
        "print(classifier(\"I feel really sad and down.\"))  # Negative emotion\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model and tokenizer saved successfully!\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Device set to use cpu\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[{'label': 'LABEL_17', 'score': 0.6141176223754883}]\n[{'label': 'LABEL_25', 'score': 0.40091267228126526}]\n"
        }
      ],
      "execution_count": 26,
      "metadata": {},
      "id": "85d3a47e-dbf7-4d93-bc3b-aad06b4d7927"
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = dataset[\"train\"].features[\"labels\"].feature.names\n",
        "print(\"Emotion labels:\", emotions)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Emotion labels: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n"
        }
      ],
      "execution_count": 28,
      "metadata": {},
      "id": "cb9521b3-ecef-4929-979a-c892c5e966dd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to get meaningful labels\n",
        "def predict_emotion(texts):\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        prediction = classifier(text)[0]  # Get the top prediction\n",
        "        label_index = int(prediction[\"label\"].split(\"_\")[-1])  # Extract numeric label index\n",
        "        emotion = emotions[label_index]  # Map to the emotion name\n",
        "        results.append(f\"Text: '{text}' -> Predicted Emotion: {emotion}, Confidence: {prediction['score']:.2f}\")\n",
        "    return results\n"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {},
      "id": "ac8a5655-2827-4b39-8387-ca2126df1f49"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GoEmotions label names\n",
        "emotions = dataset[\"train\"].features[\"labels\"].feature.names\n",
        "\n",
        "# Map the model output to emotion labels\n",
        "example_1 = classifier(\"I am so happy today!\")[0]\n",
        "example_1_label = emotions[int(example_1[\"label\"].split(\"_\")[-1])]\n",
        "print(f\"Text: 'I am so happy today!' -> Predicted Emotion: {example_1_label}\")\n",
        "\n",
        "example_2 = classifier(\"I feel really sad and down.\")[0]\n",
        "example_2_label = emotions[int(example_2[\"label\"].split(\"_\")[-1])]\n",
        "print(f\"Text: 'I feel really sad and down.' -> Predicted Emotion: {example_2_label}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text: 'I am so happy today!' -> Predicted Emotion: joy\nText: 'I feel really sad and down.' -> Predicted Emotion: sadness\n"
        }
      ],
      "execution_count": 36,
      "metadata": {},
      "id": "196fb23b-3671-42c2-97a9-201e17a7ae1e"
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier(\"This is the best day of my life!\"))\n",
        "print(classifier(\"I am feeling very anxious and nervous.\"))\n",
        "print(classifier(\"What a boring and disappointing event.\"))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[{'label': 'LABEL_0', 'score': 0.9012430310249329}]\n[{'label': 'LABEL_25', 'score': 0.1806475669145584}]\n[{'label': 'LABEL_9', 'score': 0.34697696566581726}]\n"
        }
      ],
      "execution_count": 54,
      "metadata": {},
      "id": "2f572be4-a414-4c93-b9f4-a6ca9ce8b716"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def classify_and_print(text, emotions, classifier):\n",
        "    result = classifier(text)[0] \n",
        "    label_index = int(result['label'].split('_')[-1])  # 'LABEL_0' -> 0\n",
        "    emotion = emotions[label_index]  \n",
        "    print(f\"Text: {text} -> Predicted Emotion: {emotion}, Confidence: {result['score']:.2f}\")\n",
        "\n",
        "\n",
        "emotions = dataset[\"train\"].features[\"labels\"].feature.names\n",
        "\n",
        "classify_and_print(\"I am incredibly proud of what I've accomplished.\", emotions, classifier)\n",
        "classify_and_print(\"This makes me feel so angry and frustrated.\", emotions, classifier)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text: I can't believe how amazing this is! -> Predicted Emotion: admiration, Confidence: 0.50\nText: Why does everything have to be so difficult? -> Predicted Emotion: curiosity, Confidence: 0.46\nText: I am incredibly proud of what I've accomplished. -> Predicted Emotion: admiration, Confidence: 0.91\nText: This makes me feel so angry and frustrated. -> Predicted Emotion: anger, Confidence: 0.25\n"
        }
      ],
      "execution_count": 46,
      "metadata": {},
      "id": "567a0856-3e76-4ee0-8851-dea87dc86aa8"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences_and_paragraphs = [\n",
        "    \"I am so grateful for all the support I've received!\",\n",
        "    \"This is absolutely the worst day of my life.\",\n",
        "    \"Wow, I can't stop laughing at this hilarious joke!\",\n",
        "    \"I feel so calm and relaxed when I'm by the sea.\",\n",
        "    \"It's infuriating how people can be so inconsiderate!\",\n",
        "    \"\"\"Today was one of the most exciting days of my life. I got to meet my favorite author, \n",
        "    and she even signed my book! It feels like a dream come true. I'm so thrilled and inspired \n",
        "    to start writing again after talking to her.\"\"\",\n",
        "    \"\"\"Life has been quite challenging lately. Every time I think I've made progress, another \n",
        "    obstacle appears. Sometimes, I wonder if things will ever get better. It's exhausting to \n",
        "    keep trying and failing, but I guess I have no choice but to move forward.\"\"\",\n",
        "    \"\"\"I woke up early this morning, made a cup of coffee, and sat on the balcony to watch the sunrise. \n",
        "    The world felt so peaceful, and I couldnâ€™t help but smile. These quiet moments remind me of how beautiful \n",
        "    life can be, even in its simplest forms.\"\"\"\n",
        "]\n",
        "\n",
        "\n",
        "for text in sentences_and_paragraphs:\n",
        "    classify_and_print(text, emotions, classifier)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text: I am so grateful for all the support I've received! -> Predicted Emotion: gratitude, Confidence: 0.77\nText: This is absolutely the worst day of my life. -> Predicted Emotion: sadness, Confidence: 0.17\nText: Wow, I can't stop laughing at this hilarious joke! -> Predicted Emotion: amusement, Confidence: 0.43\nText: I feel so calm and relaxed when I'm by the sea. -> Predicted Emotion: approval, Confidence: 0.13\nText: It's infuriating how people can be so inconsiderate! -> Predicted Emotion: anger, Confidence: 0.28\nText: Today was one of the most exciting days of my life. I got to meet my favorite author, \n    and she even signed my book! It feels like a dream come true. I'm so thrilled and inspired \n    to start writing again after talking to her. -> Predicted Emotion: admiration, Confidence: 0.45\nText: Life has been quite challenging lately. Every time I think I've made progress, another \n    obstacle appears. Sometimes, I wonder if things will ever get better. It's exhausting to \n    keep trying and failing, but I guess I have no choice but to move forward. -> Predicted Emotion: surprise, Confidence: 0.12\nText: I woke up early this morning, made a cup of coffee, and sat on the balcony to watch the sunrise. \n    The world felt so peaceful, and I couldnâ€™t help but smile. These quiet moments remind me of how beautiful \n    life can be, even in its simplest forms. -> Predicted Emotion: admiration, Confidence: 0.78\n"
        }
      ],
      "execution_count": 62,
      "metadata": {},
      "id": "85242661-0953-4be3-b339-5d98b953558a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences\n",
        "classify_and_print(\"I can't believe how kind everyone has been to me today.\", emotions, classifier)\n",
        "classify_and_print(\"Why does everything seem to go wrong at the worst possible time?\", emotions, classifier)\n",
        "classify_and_print(\"Iâ€™m bursting with joy after hearing this wonderful news!\", emotions, classifier)\n",
        "classify_and_print(\"This situation is making me more frustrated than Iâ€™ve ever been.\", emotions, classifier)\n",
        "classify_and_print(\"I feel so incredibly lucky to have such supportive friends.\", emotions, classifier)\n",
        "classify_and_print(\"The way they handled that issue was so unprofessional, Iâ€™m shocked!\", emotions, classifier)\n",
        "classify_and_print(\"It's such a beautiful day; I feel like everything is perfect right now.\", emotions, classifier)\n",
        "classify_and_print(\"I canâ€™t stop crying because this memory is so emotional for me.\", emotions, classifier)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text: I can't believe how kind everyone has been to me today. -> Predicted Emotion: surprise, Confidence: 0.24\nText: Why does everything seem to go wrong at the worst possible time? -> Predicted Emotion: curiosity, Confidence: 0.47\nText: Iâ€™m bursting with joy after hearing this wonderful news! -> Predicted Emotion: admiration, Confidence: 0.30\nText: This situation is making me more frustrated than Iâ€™ve ever been. -> Predicted Emotion: disappointment, Confidence: 0.17\nText: I feel so incredibly lucky to have such supportive friends. -> Predicted Emotion: joy, Confidence: 0.36\nText: The way they handled that issue was so unprofessional, Iâ€™m shocked! -> Predicted Emotion: surprise, Confidence: 0.26\nText: It's such a beautiful day; I feel like everything is perfect right now. -> Predicted Emotion: admiration, Confidence: 0.90\nText: I canâ€™t stop crying because this memory is so emotional for me. -> Predicted Emotion: sadness, Confidence: 0.25\n"
        }
      ],
      "execution_count": 64,
      "metadata": {},
      "id": "5c1e611f-4027-462d-a9fc-2f10cc1344df"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Example paragraphs\n",
        "classify_and_print(\"\"\"After a long and tiring journey, I finally reached the summit of the mountain. \n",
        "The view was breathtaking, and all the effort felt completely worth it. Standing there, I felt \n",
        "an overwhelming sense of accomplishment and peace.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"This has been the most disappointing project Iâ€™ve ever worked on. \n",
        "No one met their deadlines, and the result didnâ€™t even come close to what we expected. \n",
        "Itâ€™s hard not to feel let down after putting in so much effort.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"Last night, I had the best dinner with my family. We laughed, shared stories, \n",
        "and simply enjoyed each other's company. It reminded me how precious these moments are \n",
        "and how grateful I am for my loved ones.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"Sometimes, I feel like the weight of the world is on my shoulders. \n",
        "Between work, studies, and personal responsibilities, itâ€™s hard to find a moment to breathe. \n",
        "But deep down, I know Iâ€™ll get through it all, even if it feels overwhelming now.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"Yesterday, I got an unexpected letter from an old friend. Reading it brought back so \n",
        "many happy memories, and it felt like no time had passed since we last spoke. Itâ€™s amazing how \n",
        "some connections stay strong no matter how much time goes by.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"Spending time at the beach this weekend was pure bliss. The sound of the waves, \n",
        "the warmth of the sun, and the cool ocean breeze made me feel so relaxed and content. \n",
        "I wish I could stay there forever.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"The announcement today left me speechless. I didnâ€™t think such a big change \n",
        "would happen so suddenly, and Iâ€™m still processing it. Itâ€™s hard to say if I feel more excited \n",
        "or anxious about whatâ€™s coming next.\"\"\", emotions, classifier)\n",
        "\n",
        "classify_and_print(\"\"\"Iâ€™ve been working so hard on this project, and finally seeing it come together feels amazing. \n",
        "Every late night and tough decision has paid off, and Iâ€™m incredibly proud of what Iâ€™ve accomplished.\"\"\", emotions, classifier)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text: After a long and tiring journey, I finally reached the summit of the mountain. \nThe view was breathtaking, and all the effort felt completely worth it. Standing there, I felt \nan overwhelming sense of accomplishment and peace. -> Predicted Emotion: admiration, Confidence: 0.54\nText: This has been the most disappointing project Iâ€™ve ever worked on. \nNo one met their deadlines, and the result didnâ€™t even come close to what we expected. \nItâ€™s hard not to feel let down after putting in so much effort. -> Predicted Emotion: disappointment, Confidence: 0.35\nText: Last night, I had the best dinner with my family. We laughed, shared stories, \nand simply enjoyed each other's company. It reminded me how precious these moments are \nand how grateful I am for my loved ones. -> Predicted Emotion: joy, Confidence: 0.39\nText: Sometimes, I feel like the weight of the world is on my shoulders. \nBetween work, studies, and personal responsibilities, itâ€™s hard to find a moment to breathe. \nBut deep down, I know Iâ€™ll get through it all, even if it feels overwhelming now. -> Predicted Emotion: desire, Confidence: 0.13\nText: Yesterday, I got an unexpected letter from an old friend. Reading it brought back so \nmany happy memories, and it felt like no time had passed since we last spoke. Itâ€™s amazing how \nsome connections stay strong no matter how much time goes by. -> Predicted Emotion: admiration, Confidence: 0.81\nText: Spending time at the beach this weekend was pure bliss. The sound of the waves, \nthe warmth of the sun, and the cool ocean breeze made me feel so relaxed and content. \nI wish I could stay there forever. -> Predicted Emotion: desire, Confidence: 0.31\nText: The announcement today left me speechless. I didnâ€™t think such a big change \nwould happen so suddenly, and Iâ€™m still processing it. Itâ€™s hard to say if I feel more excited \nor anxious about whatâ€™s coming next. -> Predicted Emotion: surprise, Confidence: 0.25\nText: Iâ€™ve been working so hard on this project, and finally seeing it come together feels amazing. \nEvery late night and tough decision has paid off, and Iâ€™m incredibly proud of what Iâ€™ve accomplished. -> Predicted Emotion: admiration, Confidence: 0.88\n"
        }
      ],
      "execution_count": 66,
      "metadata": {},
      "id": "bd405395-e7da-4c54-adaf-a15a207ebe96"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "95d9dc81-ab73-4655-9e8e-d1204d6142ba"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}